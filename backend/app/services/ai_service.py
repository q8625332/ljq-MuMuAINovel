"""AIæœåŠ¡å°è£… - ç»Ÿä¸€çš„OpenAIå’ŒClaudeæ¥å£"""
from typing import Optional, AsyncGenerator, List, Dict, Any
from openai import AsyncOpenAI
from anthropic import AsyncAnthropic
from app.config import settings as app_settings
from app.logger import get_logger
import httpx

logger = get_logger(__name__)


class AIService:
    """AIæœåŠ¡ç»Ÿä¸€æ¥å£ - æ”¯æŒä»ç”¨æˆ·è®¾ç½®æˆ–å…¨å±€é…ç½®åˆå§‹åŒ–"""
    
    def __init__(
        self,
        api_provider: Optional[str] = None,
        api_key: Optional[str] = None,
        api_base_url: Optional[str] = None,
        default_model: Optional[str] = None,
        default_temperature: Optional[float] = None,
        default_max_tokens: Optional[int] = None
    ):
        """
        åˆå§‹åŒ–AIå®¢æˆ·ç«¯ï¼ˆä¼˜åŒ–å¹¶å‘æ€§èƒ½ï¼‰
        
        Args:
            api_provider: APIæä¾›å•† (openai/anthropic)ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            api_key: APIå¯†é’¥ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            api_base_url: APIåŸºç¡€URLï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            default_model: é»˜è®¤æ¨¡å‹ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            default_temperature: é»˜è®¤æ¸©åº¦ï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
            default_max_tokens: é»˜è®¤æœ€å¤§tokensï¼Œä¸ºNoneæ—¶ä½¿ç”¨å…¨å±€é…ç½®
        """
        # ä¿å­˜ç”¨æˆ·è®¾ç½®æˆ–ä½¿ç”¨å…¨å±€é…ç½®
        self.api_provider = api_provider or app_settings.default_ai_provider
        self.default_model = default_model or app_settings.default_model
        self.default_temperature = default_temperature or app_settings.default_temperature
        self.default_max_tokens = default_max_tokens or app_settings.default_max_tokens
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        openai_key = api_key if api_provider == "openai" else app_settings.openai_api_key
        if openai_key:
            # åˆ›å»ºè‡ªå®šä¹‰çš„httpxå®¢æˆ·ç«¯æ¥é¿å…proxieså‚æ•°é—®é¢˜
            try:
                # é…ç½®è¿æ¥æ± é™åˆ¶ï¼Œæ”¯æŒé«˜å¹¶å‘
                # max_keepalive_connections: ä¿æŒæ´»è·ƒçš„è¿æ¥æ•°ï¼ˆæé«˜å¤ç”¨ç‡ï¼‰
                # max_connections: æœ€å¤§å¹¶å‘è¿æ¥æ•°ï¼ˆé˜²æ­¢èµ„æºè€—å°½ï¼‰
                limits = httpx.Limits(
                    max_keepalive_connections=50,  # ä¿æŒ50ä¸ªæ´»è·ƒè¿æ¥
                    max_connections=100,            # æœ€å¤š100ä¸ªå¹¶å‘è¿æ¥
                    keepalive_expiry=30.0          # 30ç§’åè¿‡æœŸæœªä½¿ç”¨çš„è¿æ¥
                )
                
                # ä½¿ç”¨httpx.AsyncClientå¹¶è®¾ç½®è¶…æ—¶å’Œè¿æ¥æ± 
                # connect: è¿æ¥è¶…æ—¶10ç§’
                # read: è¯»å–è¶…æ—¶180ç§’ï¼ˆ3åˆ†é’Ÿï¼Œé€‚åˆé•¿æ–‡æœ¬ç”Ÿæˆï¼‰
                # write: å†™å…¥è¶…æ—¶10ç§’
                # pool: è¿æ¥æ± è¶…æ—¶10ç§’
                http_client = httpx.AsyncClient(
                    timeout=httpx.Timeout(
                        connect=10.0,
                        read=180.0,
                        write=10.0,
                        pool=10.0
                    ),
                    limits=limits
                )
                
                client_kwargs = {
                    "api_key": openai_key,
                    "http_client": http_client
                }
                
                # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„base_urlï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®
                base_url = api_base_url if api_provider == "openai" else app_settings.openai_base_url
                if base_url:
                    client_kwargs["base_url"] = base_url
                
                self.openai_client = AsyncOpenAI(**client_kwargs)
                logger.info("âœ… OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                logger.info("   - è¶…æ—¶è®¾ç½®ï¼šè¿æ¥10sï¼Œè¯»å–180s")
                logger.info("   - è¿æ¥æ± ï¼š50ä¸ªä¿æ´»è¿æ¥ï¼Œæœ€å¤§100ä¸ªå¹¶å‘")
            except Exception as e:
                logger.error(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.openai_client = None
        else:
            self.openai_client = None
            logger.warning("OpenAI API keyæœªé…ç½®")
        
        # åˆå§‹åŒ–Anthropicå®¢æˆ·ç«¯
        anthropic_key = api_key if api_provider == "anthropic" else app_settings.anthropic_api_key
        if anthropic_key:
            try:
                # ä¸ºAnthropicè®¾ç½®ç›¸åŒçš„è¶…æ—¶å’Œè¿æ¥æ± é…ç½®
                limits = httpx.Limits(
                    max_keepalive_connections=50,
                    max_connections=100,
                    keepalive_expiry=30.0
                )
                
                http_client = httpx.AsyncClient(
                    timeout=httpx.Timeout(
                        connect=10.0,
                        read=180.0,
                        write=10.0,
                        pool=10.0
                    ),
                    limits=limits
                )
                
                client_kwargs = {
                    "api_key": anthropic_key,
                    "http_client": http_client
                }
                
                # ä¼˜å…ˆä½¿ç”¨ç”¨æˆ·æä¾›çš„base_urlï¼Œå¦åˆ™ä½¿ç”¨å…¨å±€é…ç½®
                base_url = api_base_url if api_provider == "anthropic" else app_settings.anthropic_base_url
                if base_url:
                    client_kwargs["base_url"] = base_url
                
                self.anthropic_client = AsyncAnthropic(**client_kwargs)
                logger.info("âœ… Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ")
                logger.info("   - è¶…æ—¶è®¾ç½®ï¼šè¿æ¥10sï¼Œè¯»å–180s")
                logger.info("   - è¿æ¥æ± ï¼š50ä¸ªä¿æ´»è¿æ¥ï¼Œæœ€å¤§100ä¸ªå¹¶å‘")
            except Exception as e:
                logger.error(f"Anthropicå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self.anthropic_client = None
        else:
            self.anthropic_client = None
            logger.warning("Anthropic API keyæœªé…ç½®")
    
    async def generate_text(
        self,
        prompt: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            provider: AIæä¾›å•† (openai/anthropic)
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        provider = provider or self.api_provider
        model = model or self.default_model
        temperature = temperature or self.default_temperature
        max_tokens = max_tokens or self.default_max_tokens
        
        if provider == "openai":
            return await self._generate_openai(
                prompt, model, temperature, max_tokens, system_prompt
            )
        elif provider == "anthropic":
            return await self._generate_anthropic(
                prompt, model, temperature, max_tokens, system_prompt
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")
    
    async def generate_text_stream(
        self,
        prompt: str,
        provider: Optional[str] = None,
        model: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None,
        system_prompt: Optional[str] = None
    ) -> AsyncGenerator[str, None]:
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            provider: AIæä¾›å•†
            model: æ¨¡å‹åç§°
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§tokenæ•°
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        provider = provider or self.api_provider
        model = model or self.default_model
        temperature = temperature or self.default_temperature
        max_tokens = max_tokens or self.default_max_tokens
        
        if provider == "openai":
            async for chunk in self._generate_openai_stream(
                prompt, model, temperature, max_tokens, system_prompt
            ):
                yield chunk
        elif provider == "anthropic":
            async for chunk in self._generate_anthropic_stream(
                prompt, model, temperature, max_tokens, system_prompt
            ):
                yield chunk
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„AIæä¾›å•†: {provider}")
    
    async def _generate_openai(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> str:
        """ä½¿ç”¨OpenAIç”Ÿæˆæ–‡æœ¬"""
        if not self.openai_client:
            raise ValueError("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨OpenAI API")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - æ¸©åº¦: {temperature}")
            logger.info(f"  - æœ€å¤§tokens: {max_tokens}")
            logger.info(f"  - Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"  - æ¶ˆæ¯æ•°é‡: {len(messages)}")
            
            response = await self.openai_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens
            )
            
            logger.info(f"âœ… OpenAI APIè°ƒç”¨æˆåŠŸ")
            logger.info(f"  - å“åº”ID: {response.id if hasattr(response, 'id') else 'N/A'}")
            logger.info(f"  - é€‰é¡¹æ•°é‡: {len(response.choices)}")
            
            if not response.choices:
                logger.error("âŒ OpenAIè¿”å›çš„choicesä¸ºç©º")
                return ""
            
            content = response.choices[0].message.content
            logger.info(f"  - è¿”å›å†…å®¹é•¿åº¦: {len(content) if content else 0} å­—ç¬¦")
            
            if content:
                logger.info(f"  - è¿”å›å†…å®¹é¢„è§ˆï¼ˆå‰200å­—ç¬¦ï¼‰: {content[:200]}")
                return content
            else:
                logger.error("âŒ OpenAIè¿”å›äº†ç©ºå†…å®¹")
                logger.error(f"  - å®Œæ•´å“åº”: {response}")
                raise ValueError("AIè¿”å›äº†ç©ºå†…å®¹ï¼Œè¯·æ£€æŸ¥APIé…ç½®æˆ–ç¨åé‡è¯•")
            
        except Exception as e:
            logger.error(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥")
            logger.error(f"  - é”™è¯¯ç±»å‹: {type(e).__name__}")
            logger.error(f"  - é”™è¯¯ä¿¡æ¯: {str(e)}")
            logger.error(f"  - æ¨¡å‹: {model}")
            raise
    
    async def _generate_openai_stream(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨OpenAIæµå¼ç”Ÿæˆæ–‡æœ¬"""
        if not self.openai_client:
            raise ValueError("OpenAIå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        messages = []
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})
        messages.append({"role": "user", "content": prompt})
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨OpenAIæµå¼API")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"  - æœ€å¤§tokens: {max_tokens}")
            
            stream = await self.openai_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stream=True
            )
            
            logger.info(f"âœ… OpenAIæµå¼APIè¿æ¥æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æ•°æ®...")
            
            chunk_count = 0
            async for chunk in stream:
                if chunk.choices and len(chunk.choices) > 0:
                    if chunk.choices[0].delta.content:
                        chunk_count += 1
                        yield chunk.choices[0].delta.content
            
            logger.info(f"âœ… OpenAIæµå¼ç”Ÿæˆå®Œæˆï¼Œå…±æ¥æ”¶ {chunk_count} ä¸ªchunk")
            
        except httpx.TimeoutException as e:
            logger.error(f"âŒ OpenAIæµå¼APIè¶…æ—¶")
            logger.error(f"  - é”™è¯¯: {str(e)}")
            logger.error(f"  - æç¤º: è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–è€ƒè™‘ç¼©çŸ­prompté•¿åº¦")
            raise TimeoutError(f"AIæœåŠ¡è¶…æ—¶ï¼ˆ180ç§’ï¼‰ï¼Œè¯·ç¨åé‡è¯•æˆ–å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦") from e
        except Exception as e:
            logger.error(f"âŒ OpenAIæµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"  - é”™è¯¯ç±»å‹: {type(e).__name__}")
            raise
    
    async def _generate_anthropic(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> str:
        """ä½¿ç”¨Anthropicç”Ÿæˆæ–‡æœ¬"""
        if not self.anthropic_client:
            raise ValueError("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        try:
            response = await self.anthropic_client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt or "",
                messages=[{"role": "user", "content": prompt}]
            )
            return response.content[0].text
        except Exception as e:
            logger.error(f"Anthropic APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    async def _generate_anthropic_stream(
        self,
        prompt: str,
        model: str,
        temperature: float,
        max_tokens: int,
        system_prompt: Optional[str]
    ) -> AsyncGenerator[str, None]:
        """ä½¿ç”¨Anthropicæµå¼ç”Ÿæˆæ–‡æœ¬"""
        if not self.anthropic_client:
            raise ValueError("Anthropicå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼Œè¯·æ£€æŸ¥API keyé…ç½®")
        
        try:
            logger.info(f"ğŸ”µ å¼€å§‹è°ƒç”¨Anthropicæµå¼API")
            logger.info(f"  - æ¨¡å‹: {model}")
            logger.info(f"  - Prompté•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"  - æœ€å¤§tokens: {max_tokens}")
            
            async with self.anthropic_client.messages.stream(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                system=system_prompt or "",
                messages=[{"role": "user", "content": prompt}]
            ) as stream:
                logger.info(f"âœ… Anthropicæµå¼APIè¿æ¥æˆåŠŸï¼Œå¼€å§‹æ¥æ”¶æ•°æ®...")
                
                chunk_count = 0
                async for text in stream.text_stream:
                    chunk_count += 1
                    yield text
                
                logger.info(f"âœ… Anthropicæµå¼ç”Ÿæˆå®Œæˆï¼Œå…±æ¥æ”¶ {chunk_count} ä¸ªchunk")
                
        except httpx.TimeoutException as e:
            logger.error(f"âŒ Anthropicæµå¼APIè¶…æ—¶")
            logger.error(f"  - é”™è¯¯: {str(e)}")
            raise TimeoutError(f"AIæœåŠ¡è¶…æ—¶ï¼ˆ180ç§’ï¼‰ï¼Œè¯·ç¨åé‡è¯•æˆ–å‡å°‘ä¸Šä¸‹æ–‡é•¿åº¦") from e
        except Exception as e:
            logger.error(f"âŒ Anthropicæµå¼APIè°ƒç”¨å¤±è´¥: {str(e)}")
            logger.error(f"  - é”™è¯¯ç±»å‹: {type(e).__name__}")
            raise


# åˆ›å»ºå…¨å±€AIæœåŠ¡å®ä¾‹ï¼ˆä½¿ç”¨ç¯å¢ƒå˜é‡é…ç½®ï¼Œç”¨äºå‘åå…¼å®¹ï¼‰
ai_service = AIService()


def create_user_ai_service(
    api_provider: str,
    api_key: str,
    api_base_url: str,
    model_name: str,
    temperature: float,
    max_tokens: int
) -> AIService:
    """
    æ ¹æ®ç”¨æˆ·è®¾ç½®åˆ›å»ºAIæœåŠ¡å®ä¾‹
    
    Args:
        api_provider: APIæä¾›å•†
        api_key: APIå¯†é’¥
        api_base_url: APIåŸºç¡€URL
        model_name: æ¨¡å‹åç§°
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§tokens
        
    Returns:
        AIServiceå®ä¾‹
    """
    return AIService(
        api_provider=api_provider,
        api_key=api_key,
        api_base_url=api_base_url,
        default_model=model_name,
        default_temperature=temperature,
        default_max_tokens=max_tokens
    )